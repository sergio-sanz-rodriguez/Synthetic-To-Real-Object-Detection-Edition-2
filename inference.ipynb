{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Name\n",
    "\n",
    "Sergio Sanz Rodriguez\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This code implements pytorch-based testing pipeline for the Kaggle competition titled \"Synthetic to Real Object Detection Challenge.\"\n",
    "\n",
    "To run the code, please contact me via email to request access to the trained CNN models used in this project.\n",
    "\n",
    "Email: sergio.sanz.rodriguez@gmail.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "import random\n",
    "import torchvision.ops as ops\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torchvision libraries\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.io import decode_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# Import custom libraries\n",
    "from modules.obj_detection_utils import set_seeds, prune_predictions\n",
    "from modules.common import Common\n",
    "from modules.faster_rcnn import StandardFasterRCNN\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_transform_test():\n",
    "    transforms = []\n",
    "\n",
    "    # Image normalization\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "\n",
    "    # Convert to tensor and permute dimensions to (C, H, W)\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    \n",
    "    # Composition\n",
    "    return T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_select(pred1, pred2, pred3, iou_threshold=0.0):\n",
    "\n",
    "    \"\"\"\n",
    "    Merges predictions from multiple models, groups overlapping bounding boxes by IoU,\n",
    "    and selects the highest-scoring bounding box from the most frequently detected object.\n",
    "\n",
    "    Args:\n",
    "        predictions (list of dict): List of prediction dictionaries, each containing\n",
    "            'boxes' (Tensor[N, 4]), 'scores' (Tensor[N]), and 'labels' (Tensor[N]).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with a single selected prediction:\n",
    "            - 'boxes': Tensor[1, 4]\n",
    "            - 'scores': Tensor[1]\n",
    "            - 'labels': Tensor[1]\n",
    "    \"\"\"\n",
    "\n",
    "    # Merge predictions\n",
    "    all_boxes = torch.cat([pred1[\"boxes\"], pred2[\"boxes\"], pred3[\"boxes\"]], dim=0)\n",
    "    all_scores = torch.cat([pred1[\"scores\"], pred2[\"scores\"], pred3[\"scores\"]], dim=0)\n",
    "    all_labels = torch.cat([pred1[\"labels\"], pred2[\"labels\"], pred3[\"labels\"]], dim=0)\n",
    "\n",
    "    # Return empty prediction if no boxes are found\n",
    "    num_boxes = all_boxes.size(0)\n",
    "    if num_boxes == 0:\n",
    "        return {\n",
    "            \"boxes\": torch.empty((1, 4)),\n",
    "            \"scores\": torch.tensor([0.0]),\n",
    "            \"labels\": torch.tensor([0])\n",
    "        }\n",
    "\n",
    "    # Compute IoU matrix between all pairs of boxes\n",
    "    iou_matrix = ops.box_iou(all_boxes, all_boxes)\n",
    "\n",
    "    # Cluster boxes with IoU greater than the threshold\n",
    "    clusters = []\n",
    "    visited = set()\n",
    "    for i in range(num_boxes):\n",
    "        if i in visited:\n",
    "            continue\n",
    "        cluster = [i]\n",
    "        visited.add(i)\n",
    "        for j in range(i + 1, num_boxes):\n",
    "            if j not in visited and iou_matrix[i, j] > 0.01:\n",
    "                cluster.append(j)\n",
    "                visited.add(j)\n",
    "        clusters.append(cluster)\n",
    "\n",
    "    # Find the largest cluster(s) (most frequently detected object)\n",
    "    max_len = max(len(c) for c in clusters)\n",
    "    candidate_clusters = [c for c in clusters if len(c) == max_len]\n",
    "\n",
    "    # Among the candidates, select the one with the highest score\n",
    "    best_cluster = max(\n",
    "        candidate_clusters,\n",
    "        key=lambda c: all_scores[c].max().item()\n",
    "    )\n",
    "    \n",
    "    # From the best cluster, select the bounding box with the highest score\n",
    "    cluster_scores = all_scores[best_cluster]\n",
    "    best_idx_in_cluster = best_cluster[torch.argmax(cluster_scores).item()]\n",
    "\n",
    "    # Extract the final best prediction\n",
    "    best_box = all_boxes[best_idx_in_cluster].unsqueeze(0)\n",
    "    best_score = all_scores[best_idx_in_cluster].unsqueeze(0)\n",
    "    best_label = all_labels[best_idx_in_cluster].unsqueeze(0)\n",
    "\n",
    "    # Return the final prediction\n",
    "    final_pred = {\n",
    "        \"boxes\": best_box,\n",
    "        \"scores\": best_score,\n",
    "        \"labels\": best_label\n",
    "    }\n",
    "\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_object(pred1, pred2, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Compare two single-box predictions and return whether they refer to the same object.\n",
    "\n",
    "    Args:\n",
    "        pred1, pred2 (dict): Each with \"boxes\" (Tensor [1, 4]), \"scores\", \"labels\".\n",
    "        iou_threshold (float): IoU threshold to consider them the same.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if IoU > threshold, else False.\n",
    "    \"\"\"\n",
    "    iou = ops.box_iou(pred1[\"boxes\"], pred2[\"boxes\"])[0, 0]\n",
    "    return iou > iou_threshold and pred1[\"labels\"][0] == pred2[\"labels\"][0]\n",
    "\n",
    "# Now select the one with the smallest area\n",
    "def box_area(box):\n",
    "    # box is [x1, y1, x2, y2]\n",
    "    return (box[2] - box[0]) * (box[3] - box[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = r\"Synthetic_to_Real_Object_Detection_Full_2/data/test/images/*.jpg\"\n",
    "BOX_COLOR = \"blue\"\n",
    "FONT_TYPE = r\"C:\\Windows\\Fonts\\arial.ttf\"\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "model1 = StandardFasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device,\n",
    "    nms = [20, 5, 50, 2]\n",
    "    )\n",
    "model2 = StandardFasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device,\n",
    "    nms = [20, 5, 50, 2]\n",
    "    )\n",
    "model3 = StandardFasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device,\n",
    "    nms = [20, 5, 50, 2]\n",
    "    )\n",
    "model4 = StandardFasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device,\n",
    "    nms = [20, 5, 50, 2]\n",
    "    )\n",
    "\n",
    "# Load the parameters of the best model\n",
    "name1 = \"modelA1\"\n",
    "name2 = \"modelA2\"\n",
    "name3 = \"modelA3\"\n",
    "name4 = 'modelB'\n",
    "model1 = Common().load_model(model1, \"outputs\", f\"{name1}.pth\")\n",
    "model2 = Common().load_model(model2, \"outputs\", f\"{name2}.pth\")\n",
    "model3 = Common().load_model(model3, \"outputs\", f\"{name3}.pth\")\n",
    "model4 = Common().load_model(model4, \"outputs\", f\"{name4}.pth\")\n",
    "\n",
    "# Get all image paths and randomly select 10\n",
    "image_paths = glob.glob(IMAGE_DIR)\n",
    "num_images = len(image_paths)\n",
    "\n",
    "# Create subplots based on number of images\n",
    "cols = 5\n",
    "rows = (num_images + 1) // cols  # Ensure enough rows for all images\n",
    "\n",
    "# Dynamically adjust figsize based on the number of images\n",
    "fig_width = 15  # Width of the figure (you can experiment with this)\n",
    "fig_height = rows * 4  # Adjust height for better fitting\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(fig_width, fig_height))\n",
    "\n",
    "# Define transformation\n",
    "transform=get_transform_test()\n",
    "\n",
    "# Move models to device\n",
    "model1.eval().to(device)\n",
    "model2.eval().to(device)\n",
    "model3.eval().to(device)\n",
    "model4.eval().to(device)\n",
    "\n",
    "# Utility function\n",
    "def has_scores(prediction):\n",
    "    return \"scores\" in prediction and isinstance(prediction[\"scores\"], torch.Tensor) and prediction[\"scores\"].nelement() > 0\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(image_paths)\n",
    "\n",
    "# For loop over the images\n",
    "for i, image_path in enumerate(image_paths):\n",
    "\n",
    "    # Load image\n",
    "    image = decode_image(image_path)\n",
    "\n",
    "    # Image dimensions\n",
    "    img_height, img_width = image.shape[1], image.shape[2]\n",
    "    image_area = img_height * img_width\n",
    "    WIDTH = round(max(img_height, img_width) / 175)\n",
    "    FONT_SIZE = img_width // 15\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        x = transform(image)\n",
    "        x = x[:3, ...].to(device)\n",
    "        pred1 = model1([x, ])[0]     \n",
    "        pred2 = model2([x, ])[0]\n",
    "        pred3 = model3([x, ])[0]\n",
    "        pred4 = model4([x, ])[0]\n",
    "\n",
    "        # Take the best prediction(s)\n",
    "        if pred1[\"boxes\"].nelement() > 0:    \n",
    "            pred1 = prune_predictions(pred1, score_threshold = 0.8, iou_threshold = 0.01, best_candidate=\"score\", remove_large_boxes=image_area*0.5)\n",
    "        else:       \n",
    "            Common().info(f\"No prediction from model1 for {image_path}\")\n",
    "\n",
    "        # Take the best prediction(s)\n",
    "        if pred2[\"boxes\"].nelement() > 0:                \n",
    "            pred2 = prune_predictions(pred2, score_threshold = 0.8, iou_threshold = 0.01, best_candidate=\"score\", remove_large_boxes=image_area*0.5)\n",
    "        else:       \n",
    "            Common().info(f\"No prediction from model2 for {image_path}\")   \n",
    "\n",
    "        # Take the best prediction(s)\n",
    "        if pred3[\"boxes\"].nelement() > 0:                \n",
    "            pred3 = prune_predictions(pred3, score_threshold = 0.8, iou_threshold = 0.01, best_candidate=\"score\", remove_large_boxes=image_area*0.5)\n",
    "        else:       \n",
    "            Common().info(f\"No prediction from model2 for {image_path}\")\n",
    "\n",
    "        # Take the best prediction(s)\n",
    "        if pred4[\"boxes\"].nelement() > 0:                \n",
    "            pred4 = prune_predictions(pred4, score_threshold = 0.8, iou_threshold = 0.01, best_candidate=\"score\", remove_large_boxes=image_area*0.5)\n",
    "        else:       \n",
    "            Common().info(f\"No prediction from model2 for {image_path}\")                        \n",
    "\n",
    "    # Select best prediction\n",
    "    if has_scores(pred1) and has_scores(pred2) and has_scores(pred3):\n",
    "        pred = merge_and_select(pred1, pred2, pred3, iou_threshold=0.0)\n",
    "    elif has_scores(pred1):\n",
    "        pred = pred1\n",
    "    elif has_scores(pred2):\n",
    "        pred = pred2\n",
    "    elif has_scores(pred3):\n",
    "        pred = pred3\n",
    "    else:\n",
    "        pred = None\n",
    "        Common().info(f\"No prediction from any model for {image_path}\")\n",
    "\n",
    "    # Select best prediction in the second ensemble path\n",
    "    if has_scores(pred4):\n",
    "        if same_object(pred, pred4, iou_threshold=0.01) and box_area(pred4['boxes'][0]) <= box_area(pred['boxes'][0]):\n",
    "            pred = pred4\n",
    "            #print(f\"predB: {pred4['scores'].item():.4f}\")\n",
    "        #else:\n",
    "        #    preds = [pred1, pred2, pred3]\n",
    "        #    names = ['predA1', 'predA2', 'predA3']\n",
    "        #    for p, name in zip(preds, names):                \n",
    "        #        if torch.equal(p['boxes'], pred['boxes']):\n",
    "        #            best_name = name\n",
    "        #    print(f\"{best_name}: {pred['scores'].item():.4f}\")    \n",
    "\n",
    "    # Ensure predictions exist to plot the images with bounding boxes\n",
    "    if \"boxes\" in pred and pred[\"boxes\"].nelement() > 0:\n",
    "        \n",
    "        pred_labels = [f\"roi: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\n",
    "        pred_boxes = pred[\"boxes\"].long()\n",
    "        output_image = draw_bounding_boxes(\n",
    "            image,\n",
    "            pred_boxes,\n",
    "            #pred_labels,\n",
    "            colors=BOX_COLOR,\n",
    "            fill=BOX_COLOR,\n",
    "            width=WIDTH,\n",
    "            font=FONT_TYPE,\n",
    "            font_size=FONT_SIZE)   \n",
    "        image_boxes = to_pil_image(output_image)\n",
    "        \n",
    "    else:\n",
    "        image_boxes = to_pil_image(image)\n",
    "\n",
    "    # Plot image\n",
    "    ax = axes[i // cols, i % cols] if num_images > 1 else axes\n",
    "    ax.imshow(image_boxes)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Remove empty subplots if any\n",
    "total_plots = rows * cols\n",
    "for j in range(num_images, total_plots):\n",
    "    fig.delaxes(axes[j // cols, j % cols])\n",
    "\n",
    "plt.savefig(\"inference.png\", bbox_inches='tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
