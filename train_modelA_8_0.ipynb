{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Name\n",
    "\n",
    "Sergio Sanz Rodriguez\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This code implements the training, validation, and testing pipelines for the Kaggle competition titled \"Synthetic to Real Object Detection Challenge - Phase 2.\"\n",
    "\n",
    "The proposed model is based on PyTorch's Region-based CNN (R-CNN), specifically the [Faster R-CNN](https://pytorch.org/vision/master/models/faster_rcnn.html) implementation. In this competition, the ``ResNet50_FPN_v2`` backbone has been used.\n",
    "\n",
    "A key aspect of the proposed method is an ``augmentation-based regularization`` technique to improve generalization. Strong data augmentation techniques, such as horizontal and vertical flip, zooming out, occlusions, color jitter, and resolution scaling, are applied.\n",
    "\n",
    "Additional highlights of this approach include:\n",
    "\n",
    "1. ``Synthetic-only training:`` No real-world images were used during training or validation.\n",
    "2. ``No pseudo-labeling:`` The model was trained solely on the labeled synthetic data provided in the competition.\n",
    "3. ``Pre-trained model:`` A pre-trained Faster R-CNN model was fine-tuned using the Cheerios dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic libraries\n",
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from torchinfo import summary\n",
    "from pathlib import Path\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Torchvision libraries\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "# Import custom libraries\n",
    "from modules.obj_detection_utils import collate_fn, display_and_save_predictions, visualize_transformed_data, set_seeds, RandomCircleOcclusion, RandomTextureOcclusion, RandomTextureOcclusionDeterministic, BoostRedColorTransform\n",
    "from modules.obj_detection import ObjectDetectionEngine\n",
    "from modules.schedulers import FixedLRSchedulerWrapper\n",
    "from modules.common import Common\n",
    "from modules.obj_dect_dataloaders import ProcessDatasetCheerios\n",
    "from modules.faster_rcnn import StandardFasterRCNN\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch.autograd.graph\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"onnxscript.converter\")\n",
    "\n",
    "# Create target model directory\n",
    "MODEL_DIR = Path(\"outputs\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_DIR = \"FalconEDU/Scenarios/Ex2ChangeHero/Output\"\n",
    "version = \"8_0\"\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the Target Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data augmentation pipeline is used in this project to enhance model generalization. It applies several transformations, including occlusions, flipping, and zooming out. Occlusions are simulated by randomly adding colored circles and rectangles using the  ``RandomCircleOcclusion`` and ``RandomErasing`` classes.\n",
    "\n",
    "Additionally, a synthetic textures sourced from the Falcon editor's texture dataset is overlaid onto the images using the ``RandomTextureOcclusion`` class to further simulate real-world occlusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The synthetic bush texture was exported as a PNG file and integrated into the transformation pipeline as an occlusion method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Augmentation pipeline\n",
    "image_list1 = [\"images/T_Bush_Falcon.png\"]\n",
    "\n",
    "image_list2 = [\n",
    "    \"images/T_CobbleStone_Smooth_D1.png\",\n",
    "    \"images/000000016_crop.png\",\n",
    "]\n",
    "\n",
    "image_list3 = [\n",
    "    \"images/000000042_crop.png\",\n",
    "    \"images/000000061_crop.png\",\n",
    "]\n",
    "\n",
    "image_list4 = [\n",
    "    \"images/000000313_crop.png\",\n",
    "    \"images/000000459_crop.png\",\n",
    "]\n",
    "\n",
    "image_list5 = [\n",
    "    \"images/000000569_crop.png\",\n",
    "    \"images/000000856_crop.png\",\n",
    "]\n",
    "\n",
    "def get_transform_train(train, mean_std_norm=False):\n",
    "    transforms = []\n",
    "    if train:\n",
    "\n",
    "        # Resolution scaling\n",
    "        transforms.append(T.RandomChoice([T.Resize(size, interpolation=InterpolationMode.BILINEAR) for size in range(580, 1080, 250)]))\n",
    "\n",
    "        # Horizontal flip\n",
    "        transforms.append(T.RandomHorizontalFlip(p=0.5))\n",
    "\n",
    "        # Vertical flip\n",
    "        transforms.append(T.RandomVerticalFlip(p=0.5))\n",
    "\n",
    "        # Random Rotation\n",
    "        transforms.append(T.RandomRotation(degrees=(0, 90), interpolation=InterpolationMode.BILINEAR))\n",
    "\n",
    "        # Random perspective\n",
    "        #transforms.append(T.RandomAffine(degrees=0, shear=10, interpolation=InterpolationMode.BILINEAR))\n",
    "        transforms.append(T.RandomPerspective(distortion_scale=0.5, p=0.5))\n",
    "\n",
    "        # Occlusions with circles\n",
    "        transforms.append(RandomCircleOcclusion(p=0.3, scale=(0.02, 0.2), ratio=(0.3, 3.3), num_elems=3))\n",
    "\n",
    "        # Occlusions with rectangles\n",
    "        transforms.append(T.RandomErasing(p=0.5, scale=(0.02, 0.3), ratio=(0.3, 3.3), value=(0.5, 0.5, 0.5)))\n",
    "\n",
    "        # Occlusion with synthetic texture from Falcon Editor: plant\n",
    "        transforms.append(RandomTextureOcclusion(obj_path=image_list1, scale=(0.2, 0.5), transparency=1.0, p=0.5))\n",
    "        transforms.append(RandomTextureOcclusion(obj_path=image_list1, scale=(0.2, 0.5), transparency=1.0, p=0.5))\n",
    "\n",
    "        # Occlusion with synthetic texture from Falcon Editor: others\n",
    "        transforms.append(RandomTextureOcclusion(obj_path=image_list2, scale=(0.1, 0.2), transparency=1.0, p=0.5))\n",
    "\n",
    "        # Occlusion with synthetic texture from Falcon Editor: others\n",
    "        transforms.append(RandomTextureOcclusion(obj_path=image_list3, scale=(0.1, 0.2), transparency=1.0, p=0.5))\n",
    "\n",
    "        # Occlusion with synthetic texture from Falcon Editor: others\n",
    "        transforms.append(RandomTextureOcclusion(obj_path=image_list4, scale=(0.1, 0.2), transparency=1.0, p=0.5))\n",
    "\n",
    "        # Occlusion with synthetic texture from Falcon Editor: others\n",
    "        transforms.append(RandomTextureOcclusion(obj_path=image_list5, scale=(0.1, 0.2), transparency=1.0, p=0.5))\n",
    "        \n",
    "        # Color jitter\n",
    "        transforms.append(T.ColorJitter(contrast=0.3, saturation=0.3))\n",
    "\n",
    "        # Zoom out\n",
    "        transforms.append(T.RandomZoomOut(fill={tv_tensors.Image: (0.5, 0.5, 0.5), \"others\": 0}, side_range=(1.0, 1.3), p=0.5))\n",
    "\n",
    "         \n",
    "    # Image normalization\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "\n",
    "    if mean_std_norm:\n",
    "        transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "\n",
    "    # Convert to tensor and permute dimensions to (C, H, W)\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    \n",
    "    # Composition\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "def get_transform_test(train, mean_std_norm=False):\n",
    "    transforms = []\n",
    "    if train:\n",
    "        \n",
    "        # Occlusion with synthetic texture from Falcon Editor: plant\n",
    "        transforms.append(RandomTextureOcclusionDeterministic(obj_path=image_list1, scale=(0.2, 0.5), transparency=1.0, p=0.5))\n",
    "\n",
    "        # Occlusion with synthetic texture from Falcon Editor: others\n",
    "        transforms.append(RandomTextureOcclusionDeterministic(obj_path=image_list2, scale=(0.1, 0.2), transparency=1.0, p=0.5))\n",
    "\n",
    "        # Occlusion with synthetic texture from Falcon Editor: others\n",
    "        transforms.append(RandomTextureOcclusionDeterministic(obj_path=image_list3, scale=(0.1, 0.2), transparency=1.0, p=0.5))\n",
    "\n",
    "        # Occlusion with synthetic texture from Falcon Editor: others\n",
    "        transforms.append(RandomTextureOcclusionDeterministic(obj_path=image_list4, scale=(0.1, 0.2), transparency=1.0, p=0.5))\n",
    "\n",
    "        # Occlusion with synthetic texture from Falcon Editor: others\n",
    "        transforms.append(RandomTextureOcclusionDeterministic(obj_path=image_list5, scale=(0.1, 0.2), transparency=1.0, p=0.5))\n",
    "\n",
    "    # Image normalization\n",
    "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
    "\n",
    "    if mean_std_norm:\n",
    "        transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "\n",
    "    # Convert to tensor and permute dimensions to (C, H, W)\n",
    "    transforms.append(T.ToPureTensor())\n",
    "    \n",
    "    # Composition\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataloaders\n",
    "\n",
    "Below is the Python code to create the training and validation dataloaders, which will feed the object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains two classes (ROI + background)\n",
    "NUM_CLASSES = 2\n",
    "BATCHES = 4\n",
    "\n",
    "IMAGE_DIR = f\"{DATA_DIR}/data/train/images\"\n",
    "image = cv2.imread(os.path.join(IMAGE_DIR, \"000000000.png\"))\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Image dimensions\n",
    "img_height, img_width = image.shape[:2]\n",
    "img_size = (img_height, img_width)\n",
    "\n",
    "# Create the training dataset with transformations\n",
    "train_dataset = ProcessDatasetCheerios(\n",
    "    root=f\"{DATA_DIR}/data/train\",\n",
    "    image_path=\"images\",\n",
    "    label_path=\"labels\",\n",
    "    transforms=get_transform_train(train=True),\n",
    "    num_classes=NUM_CLASSES-1) # Background to be removed\n",
    "\n",
    "# Create the validation dataset with transformations\n",
    "val_dataset = ProcessDatasetCheerios(\n",
    "    root=f\"{DATA_DIR}/data/val\",\n",
    "    image_path=\"images\",\n",
    "    label_path=\"labels\",\n",
    "    transforms=get_transform_test(train=True),\n",
    "    num_classes=NUM_CLASSES-1) # Background to be removed\n",
    "\n",
    "# Create the training data loader\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Create the validation data loader\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Dataloaders: Original and Transformed\n",
    "\n",
    "Visualizing the dataloaders helps to verify that the augmentation techniques are properly applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformations\n",
    "# Original\n",
    "dataloader_ntr = torch.utils.data.DataLoader(\n",
    "    ProcessDatasetCheerios(\n",
    "        root=f\"{DATA_DIR}/data/train\",\n",
    "        image_path=\"images\",\n",
    "        label_path=\"labels\",\n",
    "        transforms=get_transform_train(train=False),\n",
    "        num_classes=NUM_CLASSES-1), # Background to be removed\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "# Transformed\n",
    "dataloader_tr = torch.utils.data.DataLoader(\n",
    "    ProcessDatasetCheerios(\n",
    "        root=f\"{DATA_DIR}/data/train\",\n",
    "        image_path=\"images\",\n",
    "        label_path=\"labels\",\n",
    "        transforms=get_transform_train(train=True),\n",
    "        num_classes=NUM_CLASSES-1), # Background to be removed\n",
    "    batch_size=BATCHES,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "# Display images\n",
    "for idx, ((img_tr, target_tr), (img_ntr, target_ntr)) in enumerate(zip(dataloader_tr, dataloader_ntr)):   \n",
    "    for i in range(0, BATCHES):\n",
    "        visualize_transformed_data(img_ntr[i], target_ntr[i], img_tr[i], target_tr[i])\n",
    "    if idx > 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Validation\n",
    "\n",
    "For this challenge, the Faster Region-based Convolutional Neural Network (Faster R-CNN) was selected as the object detection architecture. Faster R-CNN is widely recognized for delivering state-of-the-art performance in various object detection tasks, combining accuracy and efficiency through its two-stage detection pipeline.\n",
    "\n",
    "The model consists of three stages:\n",
    "\n",
    "* ``Region Proposal Network (RPN):`` The model analyzes the image to identify regions (bounding boxes) likely to contain an object. This process, known as objectness, represents the probability that a region contains an object rather than background or noise.\n",
    "* ``Classification & Bounding Box Regression:`` Once the ROIs are identified, the model classifies the objects within those regions (e.g., pedestrian, dog, table, book). Optionally, the model may also generate segmentation masks to describe the exact shape of the objects.\n",
    "* ``Bounding Box Pruning:`` In this stage bounding boxes with the lowest confidence are removed to produce cleaner outputs. It helps eliminate redundant detections.\n",
    "\n",
    "This approach provides a robust foundation for accurate object detection under the challenge's constraints, including varied occlusions and background noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = StandardFasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device,\n",
    "    nms=[20, 5, 50, 2]\n",
    "    )\n",
    "\n",
    "# Print summary\n",
    "summary(model,\n",
    "        input_size=(1,3,384, 384),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = f\"modelA_{version}.pth\"\n",
    "\n",
    "# Create AdamW optimizer\n",
    "LR = 1e-5\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=LR,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Create scheduler\n",
    "EPOCHS = 30\n",
    "scheduler = FixedLRSchedulerWrapper(\n",
    "    scheduler=CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-7),\n",
    "    fixed_lr=1e-7,\n",
    "    fixed_epoch=EPOCHS)\n",
    "\n",
    "# Instantiate the engine with the created model and the target device\n",
    "engine = ObjectDetectionEngine(\n",
    "    model=model,\n",
    "    log_verbose=True,\n",
    "    device=device)\n",
    "\n",
    "# Configure the training method\n",
    "results = engine.train(\n",
    "    target_dir=MODEL_DIR,                       # Directory where the model will be saved\n",
    "    model_name=model_name,                      # Name of the model\n",
    "    save_best_model=[\"loss\", \"last\"],           # Save the best models based on different criteria\n",
    "    keep_best_models_in_memory=False,           # Do not keep the models stored in memory for the sake of training time and memory efficiency\n",
    "    train_dataloader=train_dataloader,          # Train dataloader\n",
    "    test_dataloader=val_dataloader,             # Val dataloader\n",
    "    optimizer=optimizer,                        # Optimizer    \n",
    "    scheduler=scheduler,                        # Scheduler\n",
    "    epochs=EPOCHS,                              # Total number of epochs\n",
    "    amp=True,                                   # Enable Automatic Mixed Precision (AMP)\n",
    "    enable_clipping=False,                      # Disable clipping on gradients, only useful if training becomes unestable\n",
    "    debug_mode=False,                           # Disable debug mode    \n",
    "    accumulation_steps=1,                       # Accumulation steps: effective batch size = batch_size x accumulation steps\n",
    "    apply_validation=True                       # Enable validation step\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions on Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model file with \"model_loss_epoch\" prefix and rename it\n",
    "def rename_model(model_name: str, new_name: str):\n",
    "    old_name = model_name[0]\n",
    "    os.rename(old_name, new_name)\n",
    "    print(f\"Renamed {old_name} to {new_name}\")\n",
    "\n",
    "model_name = glob.glob(str(MODEL_DIR / f\"model_{version}_loss_epoch*.pth\"))\n",
    "new_model_name = str(MODEL_DIR / f\"model_{version}.pth\")\n",
    "rename_model(model_name, new_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the trained model\n",
    "model = StandardFasterRCNN(\n",
    "    backbone=\"resnet50_v2\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    device=device,\n",
    "    nms = [20, 5, 50, 2]\n",
    "    )\n",
    "\n",
    "# Load the parameters of the best model\n",
    "model = Common().load_model(model, \"outputs\", f\"model_{version}.pth\")\n",
    "\n",
    "# Make predictions with model.pth\n",
    "preds = ObjectDetectionEngine(\n",
    "    model=model,\n",
    "    device=device).predict(\n",
    "        dataloader=val_dataloader,\n",
    "        prune_predictions = True,\n",
    "        score_threshold = 0.8,\n",
    "        iou_threshold = 0.01,\n",
    "        best_candidate=\"score\"\n",
    "        )\n",
    "\n",
    "# Configuration parameters for visualization\n",
    "BOX_COLOR = \"blue\"\n",
    "WIDTH = round(max(img_height, img_width)/175)\n",
    "FONT_TYPE = r\"C:\\Windows\\Fonts\\arial.ttf\"\n",
    "FONT_SIZE = 48\n",
    "PRINT_LABELS = True\n",
    "\n",
    "# Display predictions\n",
    "display_and_save_predictions(\n",
    "    preds=preds,\n",
    "    dataloader=val_dataset,\n",
    "    box_color=BOX_COLOR,\n",
    "    width=WIDTH,\n",
    "    font_type=FONT_TYPE,\n",
    "    font_size=FONT_SIZE,\n",
    "    print_classes=True,\n",
    "    print_scores=True,\n",
    "    label_to_class_dict={1: 'cheerios'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on Real Data\n",
    "1. Open an ananconda terminal\n",
    "2. Activate the ``EDU`` environment\n",
    "3. Download the models from this link and copy them to folder ``outputs``\n",
    "3. Execute ``python predict_v6.py --modelA \"outputs/modelA_2_0.pth\" --modelA \"outputs/modelA_7_0.pth\" --modelA \"outputs/modelA_8_0.pth\" --modelB \"outputs/modelB_1_0.pth\"``\n",
    "4. Execute ``python convert_preds_to_csv_v3.py``\n",
    "\n",
    "To get access to the model files, please contact: sergio.sanz.rodriguez@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
